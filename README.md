# ü§ñ Chatbot de TCC (Engenharia de Software) com RAG e IA Generativa

Este projeto √© uma solu√ß√£o de **IA Generativa** desenvolvida para o desafio DIO, focada em criar um assistente virtual capaz de responder perguntas complexas com base em uma base de conhecimento privada (documentos PDF).

O cen√°rio simula um estudante de Engenharia de Software preparando seu TCC, que utiliza este chatbot para revisar e correlacionar artigos cient√≠ficos sobre DevOps, CI/CD e Microsservi√ßos.

A arquitetura central utilizada √© a **RAG (Retrieval-Augmented Generation)**, que combina busca vetorial com o poder de gera√ß√£o de linguagem dos LLMs (Google Gemini).

## üöÄ Tecnologias Utilizadas

* **Python:** Linguagem principal do projeto.
* **LangChain:** Framework para orquestrar a arquitetura RAG (LCEL).
* **Google Gemini (GenAI):** Utilizado para:
    1.  **Embeddings** (`text-embedding-004`): Transformar os textos dos PDFs em vetores.
    2.  **LLM** (`gemini-2.5-flash`): Gerar as respostas com base no contexto encontrado.
* **FAISS (Facebook AI Similarity Search):** Banco de dados vetorial local para armazenar e buscar rapidamente os *chunks* de texto relevantes.
* **PyPDF:** Biblioteca para carregar e extrair texto dos arquivos PDF.

## üõ†Ô∏è Fluxo de Execu√ß√£o (RAG)

O projeto √© dividido em duas etapas principais, refletindo a arquitetura RAG:

### 1. Processamento e Indexa√ß√£o (`process_docs.py`)

Este script √© respons√°vel por construir a base de conhecimento (o "c√©rebro" do assistente):

1.  **Load:** Carrega todos os PDFs da pasta `/docs`.
2.  **Split:** Divide os textos em fragmentos menores (*chunks*) de 1000 caracteres, com sobreposi√ß√£o de 200, para manter o contexto.
3.  **Embed:** Converte cada *chunk* de texto em um vetor num√©rico usando o Gemini.
4.  **Store:** Armazena esses vetores em um √≠ndice `FAISS` local (na pasta `/vectorstore`) para busca r√°pida por similaridade.

**[INSERIR PRINT 1 AQUI: Captura de tela do terminal executando `python process_docs.py`, mostrando a contagem de chunks (ex: 1638) e a mensagem de sucesso.]**

### 2. Chat Interativo (`chatbot.py`)

Este script executa a interface de perguntas e respostas (o RAG em a√ß√£o):

1.  **Load:** Carrega o √≠ndice FAISS salvo anteriormente.
2.  **Retrieve:** Quando o usu√°rio faz uma pergunta, o sistema busca no FAISS os 4 *chunks* de texto mais relevantes (similares) √† pergunta.
3.  **Augment (Prompting):** O sistema injeta os *chunks* encontrados (o "Contexto") em um *prompt* de sistema, instruindo o LLM a responder *apenas* com base nesse contexto.
4.  **Generate:** O LLM (Gemini) gera uma resposta em linguagem natural, fundamentada nos seus documentos.

## üìä Resultados e Testes

O chatbot demonstrou alta fidelidade ao conte√∫do dos documentos, respondendo perguntas complexas e recusando-se a responder perguntas fora do contexto dos PDFs.

**[INSERIR PRINT 2 AQUI: Captura de tela do terminal executando `python chatbot.py`, mostrando uma pergunta espec√≠fica sobre Engenharia de Software e a [RESPOSTA DA IA] com as [FONTES UTILIZADAS].]**

## üí° Insights e Aprendizados

* **O Poder do RAG:** A t√©cnica RAG √© fundamental para evitar que a IA "alucine" (invente fatos), for√ßando-a a usar apenas o conhecimento propriet√°rio fornecido (os PDFs).
* **Import√¢ncia do Chunking:** A estrat√©gia de divis√£o dos textos (tamanho e sobreposi√ß√£o) √© crucial para garantir que o contexto recuperado pela busca vetorial seja relevante.
* **Desafios de Depend√™ncia:** O desenvolvimento em IA (especialmente com LangChain) exige aten√ß√£o constante √†s vers√µes das bibliotecas, pois o ecossistema evolui muito rapidamente, causando frequentes erros de importa√ß√£o que exigem migra√ß√£o de sintaxe (como a mudan√ßa de `RetrievalQA` para LCEL).